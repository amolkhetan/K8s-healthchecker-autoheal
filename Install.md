<img width="1916" height="293" alt="image" src="https://github.com/user-attachments/assets/c61ff424-39fd-481e-b8af-3c45a6a2b67d" />

******Sprint 1******

**Cluster Set Up**

**Nodes SetUp**
1. Launch 3 EC2 machine using Ubuntu 22.04 (base image ami-03c1f788292172a4e / 10 GB) (1 master and 2 worker)
  
  base image used

  <img width="940" height="267" alt="image" src="https://github.com/user-attachments/assets/af3b3452-e847-4ec1-a76d-e35c546e8d0c" />

  ![alt text](image-1.png)

2. üß±Run below commands
   sudo apt update && sudo apt upgrade -y
   
   sudo apt install -y curl bash-completion git apt-transport-https ca-certificates gnupg lsb-release
   
   sudo swapoff -a
   
   sudo sed -i '/swap/d' /etc/fstab
   
3. üì¶Install containerd (Applicable for all nodes)
   sudo apt install -y containerd
   
   sudo mkdir -p /etc/containerd
   
   containerd config default | sudo tee /etc/containerd/config.toml

   sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
   
   sudo systemctl restart containerd

   sudo systemctl enable containerd
   
4. üß†Kernel Modules + Sysctl (Applicable for all nodes)
   cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf

   overlay

   br_netfilter

   EOF

   
   sudo modprobe overlay
   
   sudo modprobe br_netfilter

   cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf

   net.bridge.bridge-nf-call-iptables  = 1

   net.ipv4.ip_forward                 = 1

   net.bridge.bridge-nf-call-ip6tables = 1

   EOF

   sudo sysctl --system


5. üîß Install Kubernetes Components (Applicable for all nodes)
   curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | \
   gpg --dearmor | sudo tee /etc/apt/keyrings/kubernetes-apt-keyring.gpg > /dev/null

   echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | \
   sudo tee /etc/apt/sources.list.d/kubernetes.list

   sudo apt update
   sudo apt install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl

 6. üö¶ **Master Node Initialization**
    sudo kubeadm init --pod-network-cidr=10.244.0.0/16

    Post-init setup:
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

 7. üåê Pod Network (Flannel CNI)
    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

    ![alt text](image-2.png)

   AMI created for master is ami-07cd7e2b91f9e1923
   <img width="1625" height="339" alt="image" src="https://github.com/user-attachments/assets/8457d70f-a67c-45c6-aefb-926e94002d09" />


8. **Create and Add Worker Node**
   Repeat all Steps from 1-5 and the run step 8.

  sudo kubeadm join 172.31.36.143:6443 --token 8qf6he.3ghxkkgbr47ly1jj \
  --discovery-token-ca-cert-hash sha256:084ec133d4b7870c1f96a1a74a6161e5cf0b3b94e73d9bf2e7f173c2123ada83

  ![alt text](image-3.png)

  ![alt text](image-4.png)

AMI **ami-05540aeb6ec68ed3c** created for node which can we used for master or worker, according will need to run init or join comand after ec2 is launched.
<img width="1606" height="258" alt="image" src="https://github.com/user-attachments/assets/908e3765-7248-4e62-a919-ce742925a9ff" />


   Run on each worker:
   sudo kubeadm join <MASTER_IP>:6443 --token <TOKEN> \
  --discovery-token-ca-cert-hash sha256:<HASH>

   To join master node we need following details:
   - Token
      to create new (run on master): kubeadm token create
     
      to use existing (run on master): kubeadm token list
      <img width="1919" height="105" alt="image" src="https://github.com/user-attachments/assets/4979f04e-18d1-4676-8b4e-ec63c7445ed2" />
      another sample
      <img width="940" height="113" alt="image" src="https://github.com/user-attachments/assets/0af63f23-0a57-4aee-b4ae-37cecdd7b32b" />

   - MASTER_IP
       ip a | grep inet **OR** hostname -I
       <img width="1914" height="103" alt="image" src="https://github.com/user-attachments/assets/db982b2c-2cab-42b6-9b08-8991c15dbd3d" />
       another sample 
       <img width="940" height="86" alt="image" src="https://github.com/user-attachments/assets/70040745-c9d4-4f5f-bb79-71adfb0b6880" />

   - HASH
       openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | \
       openssl rsa -pubin -outform der 2>/dev/null | \
       openssl dgst -sha256 -hex | \
       sed 's/^.* //'
       <img width="1918" height="169" alt="image" src="https://github.com/user-attachments/assets/108ebf5b-365b-4eaa-92d3-0caa04b50e42" />
        another one
      <img width="940" height="128" alt="image" src="https://github.com/user-attachments/assets/14ba1ce0-a679-4f7f-8d7e-41f895a9dbd7" />

      Full Command:
     kubeadm join 172.31.32.27:6443 \
     --token kq12ua.jss372be8i7q1p9k \
     --discovery-token-ca-cert-hash sha256:a834f53ad857245d5a1919ff6049542ff7c4a5e651f003992ddd728f264721ef

      Once Joined below are screenshot to show master and worked nodes
      <img width="1913" height="279" alt="image" src="https://github.com/user-attachments/assets/608bc2f6-abc7-4234-999c-389c5adb5aea" />
      <img width="1919" height="241" alt="image" src="https://github.com/user-attachments/assets/df9f9435-498d-4a63-b649-eb4fbd686e83" />

      <img width="940" height="110" alt="image" src="https://github.com/user-attachments/assets/0c3b592c-7f90-463e-8b4e-b7830b081208" />
      <img width="940" height="249" alt="image" src="https://github.com/user-attachments/assets/6e20192e-fd6a-4114-8b6a-e97b7ee79eeb" />


**Define Folder Structure**

Ran **mkdir -p k8s-health-checker/{cmd,pkg,scripts,dashboards,manifests,alerts} && touch k8s-health-checker/{Dockerfile,README.md,go.mod,requirements.txt}**
to create below folder structure:

k8s-health-checker/
‚îú‚îÄ‚îÄ cmd/                  # Go entry points
‚îú‚îÄ‚îÄ pkg/                  # Core logic: health checks, healing actions
‚îú‚îÄ‚îÄ scripts/              # Python scripts for data processing
‚îú‚îÄ‚îÄ manifests/            # Kubernetes YAMLs
‚îú‚îÄ‚îÄ dashboards/           # Grafana JSON configs
‚îú‚îÄ‚îÄ alerts/               # Prometheus alert rules
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ go.mod / requirements.txt
‚îî‚îÄ‚îÄ README.md

sudo apt install tree

![alt text](image-8.png)


Initialize 
cd ~/k8s-health-checker

git init

**GO SetUp**
sudo apt update
sudo apt install golang-go

Verify using:
which go
go version

Set Up Go Project Directory:
mkdir -p ~/go/src/github.com/amolkhetan/k8s-health-checker
cd ~/go/src/github.com/amolkhetan/k8s-health-checker

Initialize Go Module:
go mod init github.com/your-org/k8s-health-checker


Organize Code Structure
mkdir -p cmd pkg/client
mv main.go cmd/
mv client.go pkg/client/

build Binary:
go mod tidy
go build -o health-checker ./cmd

sudo snap install docker

sudo groupadd docker
sudo usermod -aG docker $USER
newgrp docker

Build Docker File
sudo nano Dockerfile
docker build -t k8s-health-checker .
docker tag k8s-health-checker amolkhetan/k8s-health-checker:latest
docker push amolkhetan/k8s-health-checker:latest

sudo nano k8s-health-checker/manifests/rbac/service-account.yaml
sudo nano k8s-health-checker/manifests/rbac/cluster-role-binding.yaml


kubectl apply -f manifests/rbac/
kubectl apply -f manifests/deployment.yaml
kubectl apply -f manifests/deployment.yaml
kubectl apply -f manifests/deployment.yaml




sudo snap install helm --classic

**Install Prometheus**

wget https://github.com/prometheus/prometheus/releases/download/v2.50.1/prometheus-2.50.1.linux-amd64.tar.gz

tar -xvf prometheus-2.50.1.linux-amd64.tar.gz

cd prometheus-2.50.1.linux-amd64

sudo mv prometheus promtool /usr/local/bin/

sudo mkdir -p /etc/prometheus /var/lib/prometheus

sudo cp -r consoles console_libraries /etc/prometheus/

sudo nano /etc/prometheus/prometheus.yml


sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl enable prometheus
sudo systemctl start prometheus


**Node Exporter (EC2 Metrics)**
wget https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-amd64.tar.gz

tar -xvf node_exporter-1.7.0.linux-amd64.tar.gz

cd node_exporter-1.7.0.linux-amd64

sudo mv node_exporter /usr/local/bin/

sudo nano /etc/systemd/system/node_exporter.service

sudo systemctl daemon-reload
sudo systemctl enable node_exporter
sudo systemctl start node_exporter

Now, install node exporters on all nodes

wget https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-amd64.tar.gz
tar -xvf node_exporter-1.7.0.linux-amd64.tar.gz
sudo mv node_exporter-1.7.0.linux-amd64/node_exporter /usr/local/bin/

sudo useradd -rs /bin/false node_exporter

cat <<EOF | sudo tee /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
After=network.target

[Service]
User=node_exporter
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=default.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable node_exporter
sudo systemctl start node_exporter

Now test Prometheus.

**Alertmanager (Slack Notifications)**

wget https://github.com/prometheus/alertmanager/releases/download/v0.27.0/alertmanager-0.27.0.linux-amd64.tar.gz
tar -xvf alertmanager-0.27.0.linux-amd64.tar.gz
cd alertmanager-0.27.0.linux-amd64
sudo mv alertmanager amtool /usr/local/bin/
sudo mkdir -p /etc/alertmanager /var/lib/alertmanager

sudo nano /etc/alertmanager/alertmanager.yml
sudo nano /etc/systemd/system/alertmanager.service

sudo systemctl daemon-reload
sudo systemctl enable alertmanager
sudo systemctl start alertmanager

**Grafana (Dashboard UI)**

sudo apt install -y apt-transport-https software-properties-common
sudo wget -q -O /usr/share/keyrings/grafana.key https://apt.grafana.com/gpg.key
echo "deb [signed-by=/usr/share/keyrings/grafana.key] https://apt.grafana.com stable main" | \

sudo tee /etc/apt/sources.list.d/grafana.list
sudo apt update
sudo apt install grafana


sudo systemctl enable grafana-server
sudo systemctl start grafana-server

URL : http://localhost:9090

**Sprint 3**

‚úÖ 1. Implement Pod Restart & Rescheduling Based on Health Checks
üîß A. Use Liveness Probes for Auto-Restart

Update deployment yaml file to add:
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080 #change as per service port
  initialDelaySeconds: 10
  periodSeconds: 30
  failureThreshold: 3
This triggers a container restart if /healthz fails repeatedly.

üîß B. Use Readiness Probes to Control Traffic
readinessProbe:
  httpGet:
    path: /ready
    port: 8080 #change as per service port
  initialDelaySeconds: 5
  periodSeconds: 10
Prevents traffic until the pod is ready.

‚úÖ 2. Automate Cleanup of Failed Pods
üîß A. Script to Delete CrashLoopBackOff and Evicted Pods
      File is attached/present in templates
      Run this via cron or systemd timer.

‚úÖ 3. Test Self-Healing in Staging
üîß A. Simulate Failure
    kubectl exec -n slabai <pod-name> -- kill 1 (or run crashlooping pod)
    This kills the main process, triggering a restart if probes are set.

    Crash-looping pod is induced for testing purpose via crash-pod.yaml
    This was cleaned up by cleanup job deployed via pod-cleanup.yml which runs like at desired interval.

    ![alt text](image-23.png)

    ![alt text](image-24.png)
   
    ![alt text](image-25.png)

    ![alt text](image-21.png)

    ![alt text](image-26.png)

    ![alt text](image-22.png)

    ![alt text](image-16.png)
    
    ![alt text](image-17.png)    

    ![alt text](image-18.png)
üîß B. Observe Recovery
    kubectl get pods -n slabai -w
    Watch for pod restart and rescheduling.

‚úÖ 4. Log Actions for Auditing
    Logs of clean activities are being stored on pv. This pv can be accessby pvc accessor.
    kubectl exec -it -n slabai pvc-accessor -- sh
    path is /logs

******üöÄ Sprint 4 Execution Plan: Advanced Self-Healing******

****üß© 1. Automatic Node Scaling (Cluster Autoscaler)****

**Goal: Dynamically add/remove nodes based on pending pods and resource pressure.**

‚öôÔ∏è 1. Enable Node Autoscaling with Cluster Autoscaler

- Install Cluster Autoscaler:

helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set cloudProvider=aws \
  --set autoDiscovery.clusterName=<your-cluster-name> \
  --set awsRegion=<your-region>

- Tag your ASG:

k8s.io/cluster-autoscaler/enabled = true
k8s.io/cluster-autoscaler/<your-cluster-name> = owned

- IAM Permissions:
Ensure the node IAM role has permissions for EC2 Auto Scaling (e.g., autoscaling:DescribeAutoScalingGroups, autoscaling:SetDesiredCapacity).

üìà 2. Configure Horizontal Pod Autoscaler (HPA)
- Install metrics-server:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

- Add resource requests to your deployments:
resources:
  requests:
    cpu: "100m"
    memory: "128Mi"

- Create HPA:
kubectl autoscale deployment <your-deployment> \
  --cpu-percent=50 --min=2 --max=10

- Monitor:
kubectl get hpa
kubectl top pods

üß† 3. Implement Resource Balancing
- Pod Anti-Affinity

affinity:
   podAntiAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
       - labelSelector:
           matchExpressions:
             - key: app
               operator: In
               values:
                 - your-app
         topologyKey: "kubernetes.io/hostname"

- Taints and Tolerations: Use to steer workloads away from overloaded nodes.
- PriorityClasses: Ensure critical pods are scheduled first.
- Eviction Simulation:
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

üî¨ 4. Simulate Load to Test Autoscaling
kubectl run loadgen --image=busybox -- /bin/sh -c "while true; do wget -q -O- http://<service>; done"

- stress-ng:
kubectl run stress --image=alpine/stress -- stress --cpu 2 --timeout 300s


******üö® Sprint 5 Execution Plan: Alerting & Notification System******
üß© 1. Install Alertmanager (if not already installed)
This is already installed.

üì£ 2. Integrate Slack or Microsoft Teams

‚úÖ Slack Integration
1.	Create a Slack Incoming Webhook:
Create Slack account
![alt text](image-27.png)

Webhook linked created and copied in alertmanager.yml

üß∞ Step-by-Step: Create Slack Webhook URL
‚úÖ 1. Go to Slack App Management
‚Ä¢	Visit: https://api.slack.com/apps
‚Ä¢	Click ‚ÄúCreate New App‚Äù
‚Ä¢	Choose From scratch
‚Ä¢	Name your app (e.g., AlertmanagerBot) and select your workspace
________________________________________
‚úÖ 2. Enable Incoming Webhooks
‚Ä¢	In the app dashboard, go to Features ‚Üí Incoming Webhooks
‚Ä¢	Toggle Activate Incoming Webhooks to On
________________________________________
‚úÖ 3. Add a Webhook to a Channel
‚Ä¢	Scroll down and click ‚ÄúAdd New Webhook to Workspace‚Äù
‚Ä¢	Choose the channel (e.g., #alerts) where you want notifications
‚Ä¢	Click Allow
________________________________________
‚úÖ 4. Copy the Webhook URL
‚Ä¢	After approval, Slack will generate a URL like: 
‚Ä¢	
‚Ä¢	Copy this URL ‚Äî you'll use it in Alertmanager‚Äôs config
________________________________________
‚úÖ 5. Secure and Store It
‚Ä¢	Treat this URL like a secret
‚Ä¢	Store it in a Kubernetes Secret or external vault if needed


2. Restart Alertmanager

3. Test Alerts and Notifications
![alt text](image-28.png)


Sprint 6

‚úÖ Install Grafana
  Already installed

Accessed via To complete Sprint 6: Web Dashboard and Project Documentation, you‚Äôll deploy Grafana, integrate Prometheus metrics and Alertmanager logs, and document the entire system for real-world readiness. Here's your step-by-step execution plan:

üìä 1. Deploy Grafana Dashboard
‚úÖ Install Grafana via Helm:

‚úÖ Get Grafana Login Info:

‚úÖ Port-forward or expose:

Access via: http://<public-ip of master node>:3000

üìà 2. Integrate Prometheus & Alertmanager

‚úÖ Add Prometheus as a Data Source:
- Login to Grafana
- Go to Settings ‚Üí Data Sources ‚Üí Add data source
- Choose Prometheus
- Set URL to: http://localhost:9090/ (prometheus and Grafana on same host)

![alt text](image-29.png)

‚úÖ Import Dashboards:
- Go to Dashboards ‚Üí Import
- Use IDs like:
- 1860: Kubernetes cluster monitoring
- 315: Node Exporter Full
- 11074: Alertmanager

![alt text](image-30.png)

![alt text](image-31.png)



====
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install monitoring prometheus-community/kube-prometheus-stack

kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace default get pods -l "release=monitoring"

Get Grafana 'admin' user password by running:

  kubectl --namespace default get secrets monitoring-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo

Access Grafana local instance:

  export POD_NAME=$(kubectl --namespace default get pod -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=monitoring" -oname)
  kubectl --namespace default port-forward $POD_NAME 3000

Get your grafana admin user password by running:

  kubectl get secret --namespace default -l app.kubernetes.io/component=admin-secret -o jsonpath="{.items[0].data.admin-password}" | base64 --decode ; echo


Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
ubuntu@ip-172-31-25-141:~$ 

kubectl get pods -n default | grep prometheus
<img width="1176" height="179" alt="image" src="https://github.com/user-attachments/assets/c16b613e-0861-4633-8888-9fd9686eea0b" />



sudo mkdir -p k8s-health-checker/manifests/monitoring

sudo nano k8s-health-checker/manifests/monitoring/podmonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: health-checker
  labels:
    release: monitoring
spec:
  selector:
    matchLabels:
      app: health-checker
  podMetricsEndpoints:
  - port: metrics
    path: /metrics
    interval: 30s

kubectl apply -f k8s-health-checker/manifests/monitoring/podmonitor.yaml

*****Validations/Testing*****
**List all namespaces**
<img width="662" height="162" alt="image" src="https://github.com/user-attachments/assets/584e1abe-88d4-4ab4-8209-90839fcade20" />

list pods in kube-system
<img width="1054" height="203" alt="image" src="https://github.com/user-attachments/assets/7440608c-87ee-46e6-a898-7503299ebeb5" />

kubectl get nodes -o wide
kubectl get pods -n kube-system
kubectl get pods -n kube-flannel
<img width="1915" height="387" alt="image" src="https://github.com/user-attachments/assets/07403e67-46b3-4f73-97dd-0a43b1d23558" />

**Prometheus:**
kubectl get pods -n default | grep prometheus (to ensure that prometheus svc is up and running)

kubectl port-forward svc/monitoring-kube-prometheus-prometheus 9090:9090 

 ssh -i ~/Downloads/Amol-ec2.pem -L 9090:localhost:9090 ubuntu@35.91.100.196 (run on laptop bash with public ip)

 http://localhost:9090
 
 <img width="1905" height="675" alt="image" src="https://github.com/user-attachments/assets/61017873-f7d5-4c7f-9e89-4af5b89f2e0a" />

 <img width="1912" height="651" alt="image" src="https://github.com/user-attachments/assets/dc2f0aec-0bc2-4f15-a215-07d4b8adce81" />

 <img width="1911" height="941" alt="image" src="https://github.com/user-attachments/assets/230b5939-c7be-4917-9ac7-ad1ad7f6645e" />

 <img width="1906" height="913" alt="image" src="https://github.com/user-attachments/assets/07149217-056f-4f4a-9df4-ac79e7f94d33" />



API Test

<img width="1916" height="293" alt="image" src="https://github.com/user-attachments/assets/c632042a-4df6-4aa8-8e22-38743fa86173" />



kubectl port-forward svc/monitoring-kube-prometheus-prometheus 9090:9090 -n default

access it on localhost:9090
<img width="1918" height="709" alt="image" src="https://github.com/user-attachments/assets/d17f787c-0fa4-49b8-9044-2aaba48f19bf" />


For testing Grafana and Prometheus
Ran below on local where my key is present
 ssh -i ./Amol-EC2.pem -L 3000:127.0.0.1:3000 ubuntu@52.39.12.74

 It still failed to access from localhost, as 
 Grafana is trying to install plugins like grafana-lokiexplore-app, pyroscope, and exploretraces ‚Äî but failing due to no internet.

 Fix:
 nano kube-prometheus-values.yaml

 copy below in file
 
 grafana:
  plugins: []

helm upgrade monitoring prometheus-community/kube-prometheus-stack \
  -n default -f kube-prometheus-values.yaml

  or run just below 
  
  helm upgrade monitoring prometheus-community/kube-prometheus-stack \
  -n default \
  --set grafana.plugins={}

**Issues Encountered and Resoultion**
1. ‚úÖ Issue Encountered:
   ‚Ä¢	Legacy repo apt.kubernetes.io failed with 404 for both kubernetes-jammy and kubernetes-xenial.
   ‚úÖ Resolution:
   Used new official repo from pkgs.k8s.io:
   curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | \
   gpg --dearmor | sudo tee /etc/apt/keyrings/kubernetes-apt-keyring.gpg > /dev/null


===
**Install Helm Chart**
   sudo snap install helm --classic
	helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

   <img width="1918" height="218" alt="image" src="https://github.com/user-attachments/assets/d67617ee-d44e-4d46-a182-f14de98e840b" />
  
   helm repo update
   **install kube-prometheus-stack**
   helm install monitoring prometheus-community/kube-prometheus-stack
   
   **Port-forward Grafana:**
   kubectl port-forward svc/monitoring-grafana 3000:80

   <img width="1919" height="636" alt="image" src="https://github.com/user-attachments/assets/8851095a-c1e7-4e6a-b0ef-7d8983ea5471" />


**Sprint 2**


‚úÖ Final Changes to Bring Up Grafana UI on EC2
1. Initial Attempt via NodePort (Failed)
‚Ä¢	Patched Grafana service to use NodePort: 
‚Ä¢	kubectl patch svc monitoring-grafana -n default \
‚Ä¢	  -p '{"spec": {"type": "NodePort", "ports": [{"port": 80, "targetPort": 3000, "nodePort": 32000}]}}'
‚Ä¢	Verified service and public IP: 
‚Ä¢	kubectl get svc
‚Ä¢	curl http://169.254.169.254/latest/meta-data/public-ipv4
‚Ä¢	But curl http://<public-ip>:32000 failed ‚Äî no listener on port 32000.
________________________________________
2. Diagnosed NodePort Failure
‚Ä¢	Confirmed Grafana pod was healthy and exposed port 3000.
‚Ä¢	Verified service selector matched pod labels.
‚Ä¢	Ran internal test pod to access service ‚Äî DNS resolution failed.
‚Ä¢	Checked EC2 node:
‚Ä¢	sudo ss -tuln | grep 32000
‚Üí No listener.
‚Ä¢	Inspected kube-proxy and iptables:
‚Ä¢	ps aux | grep kube-proxy
‚Ä¢	iptables -t nat -L KUBE-NODEPORTS -n --line-numbers | grep 32000
‚Üí KUBE-NODEPORTS chain missing.
________________________________________
3. Attempted HostPort Exposure via Helm Deployment (Failed)
‚Ä¢	Patched Grafana deployment: 
‚Ä¢	ports:
‚Ä¢	  - containerPort: 3000
‚Ä¢	    hostPort: 3000
‚Ä¢	    name: grafana
‚Ä¢	env:
‚Ä¢	  - name: GF_SERVER_HTTP_ADDR
‚Ä¢	    value: "0.0.0.0"
‚Ä¢	Restarted pod, confirmed pod was on same node.
‚Ä¢	Verified GF_SERVER_HTTP_ADDR=0.0.0.0 inside pod.
‚Ä¢	But still no listener on host (ss -tuln | grep 3000 returned nothing).
________________________________________
4. Root Cause Identified
‚Ä¢	Helm deployment included multiple containers ‚Üí hostPort ignored.
‚Ä¢	Grafana was listening on :::3000 (IPv6 wildcard), not 0.0.0.0.
________________________________________
5. Final Fix: Standalone Deployment with HostPort
Created a minimal Grafana deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana-hostport
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana-hostport
  template:
    metadata:
      labels:
        app: grafana-hostport
    spec:
      containers:
        - name: grafana
          image: grafana/grafana:latest
          ports:
            - containerPort: 3000
              hostPort: 3000
          env:
            - name: GF_SERVER_HTTP_ADDR
              value: "0.0.0.0"
Applied it:
kubectl apply -f grafana-hostport.yaml
Verified:
sudo ss -tuln | grep 3000
curl http://<public-ip>:3000
‚úÖ Grafana UI successfully loaded.

**Testing for sprint 2**

**kubectl get nodes**
<img width="1288" height="89" alt="image" src="https://github.com/user-attachments/assets/0eaac03a-d9a8-47a4-918f-b0f8c8bb8cca" />

**kubectl get pods --all-namespaces**
<img width="1917" height="604" alt="image" src="https://github.com/user-attachments/assets/9ebb43b5-ab17-4afc-823c-f629b3a90011" />


1. kubectl port-forward svc/monitoring-kube-prometheus-prometheus 9090:9090 (EC2)
2. ssh -i ~/Downloads/Amol-ec2.pem -L 9090:localhost:9090 ubuntu@35.91.100.196 (Local)
<img width="1909" height="1035" alt="image" src="https://github.com/user-attachments/assets/8aa51e72-fcbf-4104-a7a0-04c13d9e9936" />

<img width="1910" height="939" alt="image" src="https://github.com/user-attachments/assets/9370fb7a-7d28-49fd-a478-626986f7c07e" />

<img width="1919" height="686" alt="image" src="https://github.com/user-attachments/assets/5667932a-4662-466e-b537-fd99bd607bca" />

<img width="1913" height="686" alt="image" src="https://github.com/user-attachments/assets/4d3751c8-db55-4f5e-84fd-013d1697c269" />

<img width="1919" height="960" alt="image" src="https://github.com/user-attachments/assets/dead2334-ed9c-488d-a4c8-fb16e7c96726" />

Grafana (publicip:3000)

<img width="1913" height="1028" alt="image" src="https://github.com/user-attachments/assets/3bfcba62-66d5-436e-a602-05a8e2ee6b06" />



